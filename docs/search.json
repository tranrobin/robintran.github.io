[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Homepage",
    "section": "",
    "text": "Please use the tab above to navigate through my website. Excited to have you here!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, my name is Robin. I am a senior CS and Statistics student from Mount Holyoke College. I am interested in applied statistics and machine learning. Welcome to my website!\n\nTo contact me, please reach out via LinkedIn."
  },
  {
    "objectID": "Intro-CV.html",
    "href": "Intro-CV.html",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Intro-CV.html#context",
    "href": "Intro-CV.html#context",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Context",
    "text": "Context\n\nBroader subject area: supervised learning\nWe want to build a model for some output RV \\(Y\\) given various predictor RVs \\(X_1, \\ldots, X_p\\).\nTask: regression\n\\(Y\\) is quantitative (takes numerical values)\nAlgorithm: linear regression model\nWe’ll assume that the relationship between \\(Y\\) and \\(X\\) can be represented by\n\n\\[\\mathbb{E}(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p  \\]"
  },
  {
    "objectID": "Intro-CV.html#review-k-fold-cross-validation",
    "href": "Intro-CV.html#review-k-fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Review: \\(k\\)-Fold Cross Validation",
    "text": "Review: \\(k\\)-Fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\n\n\n\nA tuning parameter is parameter or quantity upon which an algorithm depends whose value is selected or tuned to “optimize” the algorithm.\n\nFor \\(k\\)-fold CV, the tuning parameter is \\(k\\)."
  },
  {
    "objectID": "Intro-CV.html#set-up",
    "href": "Intro-CV.html#set-up",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Set Up",
    "text": "Set Up\n\nLoad Libraries\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\n# Load data\ndata(trees)\n\n\n\nRename Columns (Variables)\n\nRename Girth to diameter\nRename Height to height\n\n\n# Rename columns\ntrees = trees %&gt;% \n  rename(diameter = Girth, height = Height) %&gt;%\n  # Only have height and diameter be the columns \n  select(height, diameter)\n\n\n\nVisualization (Scatter Plot)\n\n# Create a scatter plot\nggplot(trees, aes(x = diameter, y = height)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nHeight appears to be weakly positively correlated with diameter.\n\n\n\n\n\n# Step 1: Model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  # Output Y is quantitative\n  set_mode(\"regression\") %&gt;%\n  # Want regression to be lienar\n  set_engine(\"lm\")\n\n\n# Step 2: Model estimation\ntree_model &lt;- lm_spec %&gt;% fit(height~diameter, data = trees)"
  },
  {
    "objectID": "Intro-CV.html#fold-cross-validation",
    "href": "Intro-CV.html#fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "10-Fold Cross Validation",
    "text": "10-Fold Cross Validation\n\nProcedure\n\nRandomly split the data into 10 folds\nBuilt model 10 times, leaving 1 test fold out each time\nEvaluate each model on the test fold (using MAE/MSE and R-squared as error metrics)\n\n\n# For reproducibility\nset.seed(244)\n\ntree_model_cv = lm_spec %&gt;% \n# fit_resamples() function is for fitting on folds\nfit_resamples(\n  # Specify the relationship\n  height ~ diameter, \n  # vfold_cv makes CV folds randomly from\n  # trees data set\n  resamples = vfold_cv(trees, v = 10), \n  # Specify the error metrics\n  # (MAE, square root MSE, R^2)\n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ntree_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   4.29     10  0.643  Preprocessor1_Model1\n2 rmse    standard   5.12     10  0.790  Preprocessor1_Model1\n3 rsq     standard   0.508    10  0.0958 Preprocessor1_Model1\n\n\n\n\nSummarizing\n\n# Get fold-by-fold results\n# Get info for each test fold \ntree_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [27/4]&gt; Fold01 mae     standard        5.67 Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [28/3]&gt; Fold02 mae     standard        3.16 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [28/3]&gt; Fold03 mae     standard        7.60 Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [28/3]&gt; Fold04 mae     standard        3.27 Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [28/3]&gt; Fold05 mae     standard        1.32 Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [28/3]&gt; Fold06 mae     standard        4.75 Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [28/3]&gt; Fold07 mae     standard        3.58 Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [28/3]&gt; Fold08 mae     standard        1.70 Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [28/3]&gt; Fold09 mae     standard        5.62 Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [28/3]&gt; Fold10 mae     standard        6.28 Preprocessor1_Mo… &lt;tibble&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 7 and worst for fold 3."
  },
  {
    "objectID": "Intro-CV.html#exercise-1-in-sample-metrics",
    "href": "Intro-CV.html#exercise-1-in-sample-metrics",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 1: In-Sample Metrics",
    "text": "EXERCISE 1: In-Sample Metrics\nUse the health_data data to build two separate models of height:\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = health_data)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1   0.366   0.272  6.13    3.92 0.00650     5  -126.  266.  278.   1279.      34\n# … with 1 more variable: nobs &lt;int&gt;, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1   0.526   0.229  6.31    1.77   0.102    15  -120.  274.  303.    956.      24\n# … with 1 more variable: nobs &lt;int&gt;, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\nANSWER. The R2 value for the first model is about 0.366, and for the second model, it is 0.526.\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        3.48\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        3.37"
  },
  {
    "objectID": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "href": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 2: In-Sample Model Comparison",
    "text": "EXERCISE 2: In-Sample Model Comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\nBased on the in-sample MAE, it appears that model 2 (whose MAE is about 3.366) is better than model 1 (whose MAE is about 3.48)."
  },
  {
    "objectID": "Intro-CV.html#exercise-3-10-fold-cv",
    "href": "Intro-CV.html#exercise-3-10-fold-cv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 3: 10-Fold CV",
    "text": "EXERCISE 3: 10-Fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight + thigh + knee + ankle\nmodel_2: height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )"
  },
  {
    "objectID": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "href": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 4: Calculating the CV MAE",
    "text": "EXERCISE 4: Calculating the CV MAE\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   4.13     10  0.897  Preprocessor1_Model1\n2 rsq     standard   0.288    10  0.0852 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   6.28     10   0.921 Preprocessor1_Model1\n2 rsq     standard   0.257    10   0.115 Preprocessor1_Model1\n\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\nANSWER. We expect our first model to produce predictions of height that are roughly off by 4.13 (the observed MAE) on average. For the first model, we expect it to explain roughly 0.28 (28%) of the variability (based on the R2 value) in the observed heights of patients in the dataset."
  },
  {
    "objectID": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "href": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 5: Fold-By-Fold Results",
    "text": "EXERCISE 5: Fold-By-Fold Results\nThe command collect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. The command unnest(.metrics) provides the MAE from each test fold.\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%\n  filter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [36/4]&gt; Fold01 mae     standard       3.34  Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [36/4]&gt; Fold02 mae     standard       0.820 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [36/4]&gt; Fold03 mae     standard       2.11  Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [36/4]&gt; Fold04 mae     standard       5.16  Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [36/4]&gt; Fold05 mae     standard      10.9   Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [36/4]&gt; Fold06 mae     standard       2.25  Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [36/4]&gt; Fold07 mae     standard       4.11  Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [36/4]&gt; Fold08 mae     standard       2.91  Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [36/4]&gt; Fold09 mae     standard       6.18  Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [36/4]&gt; Fold10 mae     standard       3.52  Preprocessor1_Mo… &lt;tibble&gt;\n\n\n\nWhich fold had the worst average prediction error and what was it?\n\n\nFor me, fold 5 had the worst MAE.\n\n\nRecall that collect_metrics() reported a final CV MAE of ___ for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\n# Code here\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;%\n  filter(.metric == \"mae\") %&gt;%\n  summarize(mean(.estimate))\n\n# A tibble: 1 × 1\n  `mean(.estimate)`\n              &lt;dbl&gt;\n1              4.13"
  },
  {
    "objectID": "Intro-CV.html#exercise-6-comparing-models",
    "href": "Intro-CV.html#exercise-6-comparing-models",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 6: Comparing Models",
    "text": "EXERCISE 6: Comparing Models\nFill in the table below to summarize the in-sample and 10-fold CV MAE for both models.\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n3.48\n4.13\n\n\nmodel_2\n3.37\n6.28\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on the CV MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on all of these results, which model would you pick?\n\n\nYOUR ANSWER HERE\n\n\nDo the in-sample and CV MAE suggest that model_1 is overfit to our health_data sample data? What about model_2?\n\n\nFor Model 1, it looks like the MAE is roughly similar for when it is measured in-sample versus when it is tested on new data. However, Model2 seems overfit because its predictions for new patient data are much worse than its predictions for patients in our data sample."
  },
  {
    "objectID": "Intro-CV.html#exercise-7-loocv",
    "href": "Intro-CV.html#exercise-7-loocv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 7: LOOCV",
    "text": "EXERCISE 7: LOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our health_data sample?\n\n\n# CODE HERE\nset.seed(244)\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = 40), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_loocv %&gt;% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard     4.30    40   0.981 Preprocessor1_Model1\n2 rsq     standard   NaN        0  NA     Preprocessor1_Model1\n\n\n\nHow does the LOOCV MAE compare to the 10-fold CV MAE of ___? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\n\nANSWER.\n\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\nANSWER."
  },
  {
    "objectID": "Lab9_Lasso.html",
    "href": "Lab9_Lasso.html",
    "title": "Lab 9: LASSO",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 9: LASSO",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(y ~ ., data = sample_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(___, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = ___),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab9_Lasso.html#tuning-lambda",
    "href": "Lab9_Lasso.html#tuning-lambda",
    "title": "Lab 9: LASSO",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 9: LASSO",
    "section": "LASSO: least absolute shrinkage and selection operator",
    "text": "LASSO: least absolute shrinkage and selection operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\n\nCOMMENT: Picking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-1",
    "href": "Lab9_Lasso.html#exercise-1",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 1",
    "text": "EXERCISE 1\nLet’s compare the CV MAEs (y-axis) for our 50 LASSO models which used 50 different \\(\\lambda\\) values (x-axis):\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\n\n\n\n\n\n\n\n\nWe told R to use a range of \\(\\lambda\\) from -5 to -0.1 on the log10 scale. Calculate this range on the non-log scale and confirm that it matches the x-axis.\n\n\n10^(-5)\n\n[1] 1e-05\n\n\n\n10^(-.1)\n\n[1] 0.7943282\n\n\nWhat can we observe from this plot?\n\nANSWER. The mae decreases as \\(\\lambda\\) increases."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-2",
    "href": "Lab9_Lasso.html#exercise-2",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\n\nIn the plot above, roughly which value of the \\(\\lambda\\) penalty parameter produces the smallest CV MAE?\n\n\nANSWER. \\(\\lambda\\) = 4 seems to produces the smallest CV MAE. The ideal \\(\\lambda\\) is around 3.1.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1    3.16 Preprocessor1_Model50\n\n\n\nSuppose we prefer a parsimonious model.\n\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5)\n\n\n\n\n\n\n\n\nUse this to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER. \\(\\lambda\\) = 3 seems to produces the smallest CV MAE.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n# A tibble: 1 × 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1    3.16 mae     standard    3.69    10    1.00 Preprocessor1_Mod…  3.69   4.69\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\n\nANSWER. The ideal \\(\\lambda\\) is around 3.16.\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\nEXERCISE 3\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(-0.1) and see what you observe.\n\nANSWER"
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "href": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 4: Finalizing our LASSO model",
    "text": "EXERCISE 4: Finalizing our LASSO model\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = health_data)\n\nfinal_lasso %&gt;% \n  tidy()\n\n# A tibble: 13 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)     68.9    3.16\n 2 age              0      3.16\n 3 weight           0      3.16\n 4 neck             0      3.16\n 5 chest            0      3.16\n 6 abdomen          0      3.16\n 7 hip              0      3.16\n 8 thigh            0      3.16\n 9 knee             0      3.16\n10 ankle            0      3.16\n11 biceps           0      3.16\n12 forearm          0      3.16\n13 wrist            0      3.16\n\n\n\nHow many and which predictors were kept in this model?\n\n\nANSWER.\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html",
    "href": "Lab10_Lasso_Cont.html",
    "title": "Lab 10: LASSO (Continued)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab10_Lasso_Cont.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(y ~ ., data = sample_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(___, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = ___),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#tuning-lambda",
    "href": "Lab10_Lasso_Cont.html#tuning-lambda",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#context",
    "href": "Lab10_Lasso_Cont.html#context",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Context",
    "text": "Context\n\nWorld = supervised learning\nWe want to model some output variable \\(Y\\) using a set of potential predictors (\\(X_1, X_2, ..., X_p\\)).\nTask = regression\n\\(y\\) is quantitative\nModel = linear regression\nWe’ll assume that the relationship between \\(Y\\) and (\\(X_1, X_2, ..., X_p\\)) can be represented by the model equation\n\\[\\mathbb{E}(Y \\mid X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nand corresponding regression equation\n\\[\\hat{Y} = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\cdots + \\hat\\beta_p X_p \\]\nEstimation algorithm = LASSO"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab10_Lasso_Cont.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO: Least Absolute Shrinkage and Selection Operator",
    "text": "LASSO: Least Absolute Shrinkage and Selection Operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\nPicking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#exercises",
    "href": "Lab10_Lasso_Cont.html#exercises",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Exercises",
    "text": "Exercises\nWe will use the LASSO algorithm to help us build a good predictive model of height using the collection of 12 possible predictors in the new_health_data data set:\nLet’s implement the LASSO. We’ll pause and adjust the code from the R Code Notes for LASSO section.\n\n# STEP 1: LASSO Algorithm & Model Specification\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_spec &lt;- linear_reg() %&gt;%             \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;%                 \n  set_args(mixture = 1, penalty = tune()) \n\n\n# STEP 2: Variable Recipe\n# y ~. for response y \nvariable_recipe &lt;- recipe(height ~ ., data = new_health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\n\n# STEP 4: Estimate 50 LASSO models using \n# lambda values on a \"grid\" or range from 10^(-5) to 10^(-0.1).\n# Calculate the 10-fold CV MAE for each of the 50 models.\n# Note: I usually start with a range from 10^(-5) to 10^1 and \n# tweak through trial-and-error.\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, .1)), levels = 50),\n    resamples = vfold_cv(new_health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nLast time: The code below finds the “best” \\(\\lambda\\) (the \\(\\lambda\\) for which the CV MAE is smallest).\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0346 Preprocessor1_Model35\n\n\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\n\n\n\n\n\n\n\n\nParsimonious Model\nSuppose we prefer a parsimonious model.\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err),\n                alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nUse the plot with error bars to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER. The minimum MAE is around 1.55 at a very small \\(\\lambda\\) = 0.1.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n# A tibble: 1 × 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1   0.115 mae     standard    1.71    10   0.174 Preprocessor1_Mod…  1.58   1.77\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\nANSWER. The tuned \\(\\lambda\\) is 0.114.\n\n\n\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(1) and see what you observe. Adjust the range until it seems appropriate (no flat lines, but no minimum on a boundary).\n\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, 1)), levels = 50),\n    resamples = vfold_cv(new_health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0356 Preprocessor1_Model30\n\n\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err),\n                alpha = 0.5)\n\n\n\n\n\n\n\n\n\nANSWER. The minimum MAE is at a very small lambda, close to 0. There seems to be no boundaries.\n\n\n\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = new_health_data)\n\nfinal_lasso %&gt;% \n  tidy()\n\n# A tibble: 13 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 74.3       0.115\n 2 age         -0.00234   0.115\n 3 weight       0.124     0.115\n 4 neck        -0.181     0.115\n 5 chest        0         0.115\n 6 abdomen     -0.190     0.115\n 7 hip          0         0.115\n 8 thigh       -0.136     0.115\n 9 knee         0.161     0.115\n10 ankle        0.0230    0.115\n11 biceps       0         0.115\n12 forearm      0         0.115\n13 wrist        0         0.115\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nLet’s finalize our LASSO model.\n\nHow many and which predictors were kept in this model?\n\n\nANSWER. 9\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER. I think that the goal of LASSO is to predict, not to interpret. It applies penalty, so that the interpretation might not be correct.\n\n\n\nOur parsimonious LASSO selected only 7 of the 12 possible predictors. Out of curiosity, how many predictors would have remained if we had used the best_penalty value for \\(\\lambda\\)?\n\nlasso_workflow %&gt;% \n  finalize_workflow(parameters = best_penalty) %&gt;% \n  fit(data = new_health_data) %&gt;% \n  tidy() %&gt;% \n  filter(estimate != 0)\n\n# A tibble: 11 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  94.0     0.0356\n 2 age          -0.0171  0.0356\n 3 weight        0.207   0.0356\n 4 neck         -0.486   0.0356\n 5 chest        -0.0691  0.0356\n 6 abdomen      -0.206   0.0356\n 7 thigh        -0.281   0.0356\n 8 knee          0.162   0.0356\n 9 biceps       -0.110   0.0356\n10 forearm      -0.0410  0.0356\n11 wrist        -0.0150  0.0356\n\n\n\nANSWER. 9\n\nBased on this example, do you think LASSO is a greedy algorithm? Are you “stuck” with your past locally optimal choices? Compare the predictors in this larger model with those in the smaller, parsimonious model.\n\nANSWER. LASSO is not a greedy algorithm. I don’t think that we will be stuck with our past locally optimal choices."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#lasso-vs-least-squares",
    "href": "Lab10_Lasso_Cont.html#lasso-vs-least-squares",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO vs Least Squares",
    "text": "LASSO vs Least Squares\nLet’s compare our final_lasso model to the least squares model using all predictors.\n\n# Build the LS model\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nls_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(variable_recipe)\n\nls_model &lt;- ls_workflow %&gt;% \n  fit(data = new_health_data) \n\n# examine coefficients\nls_model %&gt;% \n  tidy()\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic     p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 (Intercept) 110.       16.9       6.51   0.000000561\n 2 age          -0.0234    0.0367   -0.637  0.529      \n 3 weight        0.266     0.0573    4.64   0.0000810  \n 4 neck         -0.671     0.335    -2.00   0.0556     \n 5 chest        -0.119     0.131    -0.908  0.372      \n 6 abdomen      -0.196     0.113    -1.73   0.0946     \n 7 hip          -0.0978    0.189    -0.518  0.609      \n 8 thigh        -0.313     0.163    -1.92   0.0661     \n 9 knee          0.199     0.272     0.733  0.470      \n10 ankle        -0.0262    0.449    -0.0583 0.954      \n11 biceps       -0.168     0.200    -0.837  0.410      \n12 forearm      -0.0770    0.144    -0.536  0.596      \n13 wrist        -0.0962    0.647    -0.149  0.883      \n\n# get 10-fold CV MAE\nset.seed(123)\nls_workflow %&gt;% \n  fit_resamples(\n    resamples = vfold_cv(new_health_data,v = 10),\n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    1.59    10   0.221 Preprocessor1_Model1\n\n\n\nOur final_lasso has 9 predictors and a CV MAE of 1.594 (calculated above). The ls_model has 12 predictors and a CV MAE of 1.714941.\n\n\nANSWER. Our final_lasso has 9 predictors and a CV MAE of 1.594 (calculated above). The ls_model has 12 predictors and a CV MAE of 1.714941.\n\n\nUse both final_lasso and ls_model to predict the height of the new patient below. How do these compare? Does this add to or calm any fears you might have had about shrinking coefficients?!\n\n\nnew_patient &lt;- data.frame(age = 50, weight = 200, neck = 40, \n                          chest = 115, abdomen = 105, hip = 100, \n                          thigh = 60, knee = 38, ankle = 23, biceps = 32, \n                          forearm = 29, wrist = 19) \n\n\n# LS prediction\nls_model %&gt;% \n  predict(new_data = new_patient)\n\n\n# LASSO prediction\nfinal_lasso %&gt;% \n  predict(new_data = new_patient)\n\n\nWhich final model would you choose, the LASSO or least squares?\n\n\nANSWER. I would choose the LASSO model because it simplifies the model."
  },
  {
    "objectID": "Lab8_VariableSubsetSelection.html",
    "href": "Lab8_VariableSubsetSelection.html",
    "title": "Variable Subset Selection",
    "section": "",
    "text": "Best Subset Selection (Notes)\n\nWith \\(p\\) predictors, there are \\(2^p\\) possible models because there are \\(2^p\\) possible subsets of the \\(p\\) predictors. (Size of the powerset) – alternatively, two choices for each predictor, either include or not include\nIn best subset selection procedure, we fit all \\(2^p\\) models and choose the one with the best value of our chosen evaluation metric (e.g., test error estimate based on CV, using some dedicated test set, etc.). Our “evaluation metric” (equivalently, “error metric”) should fairly assess test performance of model (as opposed to training set performance)\n\\(2^p\\) models? This sounds expensive.\nExample: \\(p = 4\\), 16 potential models to fit (including the zero variable model, i.e., one with just an intercept)\nDoesn’t try to transform variables or include interaction terms – would have to manually include transformed variables or interaction models in model\n\n\n\nForward Stepwise Selection\n\nIdea: add variables one at a time, choosing the “best” variable each time\n\nThis is known as a greedy algorithm: goes for temporarily/locally best choice without thinking about some kind of long-term optimum\n\nHistorical note: many past (and current) implementations to determine which variable is “best” is to choose the one whose inclusion had the lowest p-value. Problematic: discussed at the end.\n\nBetter to use CV estimates of test error instead of choices based on p-values\n\n\n\n\nGeneral Notes\n\nBackwards and forwards selection can give different results due to greedy behavior of each\nWhen several “reasonable” methods (e.g., forward/backward selection if best subset is prohibitively expensive), best practice is to try all methods, compare their results, and report all results\n\n\n\nCautions\n\nSome machine learning practitioners don’t like automated selection methods, b/c encourage us to not think about the variables b/c can just dump data into an algorithm and get a model\nNeed to think of variables in context, and carelessly using automated procedures can do real harm\nUsing p-values for deciding which variables to add/remove results gives unstable (and often “undesireable”) results when several quantitative predictors are correlated with one another (called collinearity – e.g., one predictor is a linear combination of others) – not the same as a violated independence assumption\n\nStatistical inference problems on final selected model: chosen through multiple hypothesis testing * Because we come to select a final model by trying a large number of models, multiple hypothesis testing is a big issue.\n\nWith multiple testing, the idea is that testing many hypotheses runs the risk of a result being statistically significant just by chance.\nMight have to do a Bonferroni correction or something like that (ignore this comment if you haven’t taken STAT 242)\n\nCIs are misleadingly narrow (from multiple hyp. testing)\nP-values are misleadingly small (from mult. hyp testing)\n\n\n\n\nSummary\nSubset selection methods use model quality metrics to search through different models in an automated way\n\nBest subset selection: intuitive, accurate, quickly computationally expensive\nStepwise selection (either forward or backwards): faster but not guaranteed to find “best model” due to greedy nature\nNo methods automatically consider transformations or interactions (we manually include those to exlore them)\nProblems with statistical inference\n\nShrinkage/regularization methods are a popular alternative to subset selection.\n\n\nFor Fun: Heat Map for Exploring Multicollinearity\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\n# Load data\nhealth_data = read_xlsx(\"healthdata.xlsx\")\nhead(health_data)\n\n# A tibble: 6 × 13\n    age weight  neck chest abdomen   hip thigh  knee ankle biceps forearm wrist\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    42   136.  37.8  87.6    77.6  88.6  51.9  34.9  22.5   27.7    27.5  18.5\n2    61   143   36.5  93.4    83.3  93    55.5  35.2  20.9   29.4    27    16.8\n3    49   213.  40.8 105.    107.  108.   66.5  42.5  24.5   35.5    29.8  18.7\n4    40   168.  34.2  97.8    92.3 101.   57.5  36.8  22.8   32.1    26    17.3\n5    52   199.  39.4 107.    100   105    63.9  39.2  22.9   35.7    30.4  19.2\n6    55   125   33.2  87.7    76    88.6  50.9  35.4  19.1   29.3    25.7  16.9\n# … with 1 more variable: height &lt;dbl&gt;\n\n\n\n# Get the correlation matrix\nlibrary(reshape2)\ncor_matrix &lt;- cor(health_data)\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n# STEP 1: Model Specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: Model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\n# Look at model\nheight_model_1 %&gt;% tidy()\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 125.        61.9       2.02   0.0531\n 2 age          -0.112      0.132    -0.847  0.405 \n 3 weight        0.379      0.213     1.78   0.0864\n 4 neck          0.139      1.17      0.119  0.906 \n 5 chest        -0.459      0.473    -0.971  0.340 \n 6 abdomen       0.283      0.354     0.798  0.432 \n 7 hip          -0.921      0.510    -1.81   0.0822\n 8 thigh        -1.24       0.646    -1.92   0.0660\n 9 knee          0.151      0.941     0.160  0.874 \n10 ankle        -0.888      1.28     -0.693  0.494 \n11 biceps       -0.0808     0.746    -0.108  0.915 \n12 forearm       2.25       1.80      1.25   0.223 \n13 wrist         0.836      2.32      0.361  0.721 \n\n\n\n\nBest Subset Selection Algorithm\n\nBuild all possible models that use any combination of the available predictors\nIdentify the best model with respect to some chosen metric (eg: CV MAE, CV MSE) and context.\n\n\n\nExercise\nSuppose we used this algorithm for our height model with all 12 possible predictors (13 variables total, one is height, leaving 12 predictors). What’s the main drawback?\n\nANSWER. The main drawback here is that the model can be overfitting. Using all 12 predictors can make the model too complex and less accurate on new data.\n\n\n\nBackward Stepwise Selection Algorithm\n\nBuild a model with all \\(p\\) possible predictors,\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. In a future HW assignment, you will implement the above using MSE/MAE error metrics.\nFor this example though, for simplicity, we will identify the “best” model with respect to p-values.\nLet’s try out the first few steps!\nFirst, we would compute the 10-fold MAE of the model with all 12 predictors.\n\nset.seed(244)\nmodel_12_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nmodel_12_predictors_cv %&gt;%  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    5.56    10   0.840 Preprocessor1_Model1\n\n\nThen let’s look at which predictor to remove to identify the “best” model with 11 predictors.\n\n# Original model with 12 predictors\n# Find the \"least significant\" predictor\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 12 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age      -0.112      0.132    -0.847  0.405 \n 2 weight    0.379      0.213     1.78   0.0864\n 3 neck      0.139      1.17      0.119  0.906 \n 4 chest    -0.459      0.473    -0.971  0.340 \n 5 abdomen   0.283      0.354     0.798  0.432 \n 6 hip      -0.921      0.510    -1.81   0.0822\n 7 thigh    -1.24       0.646    -1.92   0.066 \n 8 knee      0.151      0.941     0.160  0.874 \n 9 ankle    -0.888      1.28     -0.693  0.494 \n10 biceps   -0.0808     0.746    -0.108  0.915 \n11 forearm   2.25       1.80      1.25   0.223 \n12 wrist     0.836      2.32      0.361  0.721 \n\n\nLooks like we should remove biceps.\nWe would then compute the 10-fold MAE of the model with 11 predictors.\n\n# 11 predictors\n# UPDATE this code\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 11 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age       -0.111     0.130    -0.858  0.398 \n 2 weight     0.369     0.190     1.94   0.0623\n 3 neck       0.161     1.14      0.142  0.888 \n 4 chest     -0.453     0.461    -0.982  0.334 \n 5 abdomen    0.281     0.348     0.809  0.425 \n 6 hip       -0.902     0.470    -1.92   0.0652\n 7 thigh     -1.26      0.602    -2.09   0.0454\n 8 knee       0.180     0.886     0.203  0.841 \n 9 ankle     -0.878     1.26     -0.699  0.490 \n10 forearm    2.17      1.62      1.34   0.192 \n11 wrist      0.907     2.18      0.416  0.681 \n\n\nLooks like we should remove neck.\n\n# 10 predictors\n# UPDATE this code\nlm_spec %&gt;% \n  fit(height ~ age + weight + chest + abdomen + hip + thigh + ankle + knee + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 10 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age       -0.111     0.127    -0.869  0.392 \n 2 weight     0.377     0.179     2.11   0.0435\n 3 chest     -0.460     0.451    -1.02   0.316 \n 4 abdomen    0.298     0.322     0.924  0.363 \n 5 hip       -0.931     0.416    -2.24   0.0331\n 6 thigh     -1.26      0.591    -2.14   0.0409\n 7 ankle     -0.884     1.23     -0.716  0.480 \n 8 knee       0.166     0.866     0.191  0.850 \n 9 forearm    2.29      1.37      1.66   0.107 \n10 wrist      0.985     2.07      0.475  0.639 \n\n\n\n\nBackward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(244)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n___\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n___\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n___\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.368\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight"
  },
  {
    "objectID": "FPCP3.html",
    "href": "FPCP3.html",
    "title": "FPCP3",
    "section": "",
    "text": "data &lt;- read_csv(\"customer.csv\")\ndata &lt;- as.data.frame(data)\n\n\nPart 1: Cross Validation (CV)\n\nBrief Introduction to Cross-Validation\n\nCross-validation is a model evaluation method that estimates test error by partitioning the data into training and validation sets. In \\(k\\)-fold cross-validation, we divide the data into \\(k\\) roughly equal parts (folds). For each fold \\(j = 1, \\dots, k\\), we:\n\nTrain the model on the other \\(k - 1\\) folds,\nPredict the outcomes for fold \\(j\\),\nCompute the prediction error (e.g., MAE or MSE).\n\nThe overall cross-validation error is the average over all folds:\n\\[\n\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^{k} \\text{Err}_j\n\\]\nIn Leave-One-Out Cross-Validation (LOOCV), \\(k = n\\), so we leave out one observation at a time.\n\nGoal of Cross-Validation\n\nThe goal of cross-validation is to estimate how well a model will perform on new, unseen data. It helps us detect overfitting by testing the model on different subsets of the data that were not used during training. This makes the evaluation more reliable than just using one random train-test split. Using cross-validation also gives a more fair and stable estimate of how the model will perform in general, instead of depending on just one train-test split.\n\nLinear Models\n\n\nlm_spec &lt;- \n  linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n\nmodel_1 &lt;- lm_spec %&gt;% fit(avg_transaction_value ~ age + gender + region_category + membership_category, data = data)\n\nThis model includes basic demographic and membership info to see if certain types of customers are more likely to churn.\n\nmodel_2 &lt;- lm_spec %&gt;% fit(avg_transaction_value ~ avg_time_spent + avg_frequency_login_days + days_since_last_login + points_in_wallet, data = data)\n\nThis model focuses on recent customer behavior, since more active users may have lower churn risk.\n\nmodel_3 &lt;- lm_spec %&gt;% fit(avg_transaction_value ~ age*gender + avg_time_spent + membership_category + past_complaint + internet_option + points_in_wallet, data = data)\n\nThis model combines features from all domains and includes interaction between age and gender.\n\nTrain-Test Split\n\nBefore fitting any models, I divided the dataset into a training set and a test set using an 80/20 split. The training set would be used to perform cross-validation, and the test set would be used to evaluate final model performance.\n\nset.seed(244)\ndata_split &lt;- initial_split(data, prop = 0.8)\ndata &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nError Metric: MAE\n\nI chose Mean Absolute Error (MAE) as the primary error metric. Mathematical definition of MAE is\n\\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (y_1 - \\hat{y}_1, \\dots, y_n - \\hat{y}_n)\n\\]\nLet \\[ \\mathbf{e} \\] be the error vector.\nThen\n\\[\n\\text{MAE}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} | y_i - \\hat{y}_i | = \\frac{ \\|\\mathbf{e}\\|_1 }{n_j}\n\\]\nMAE is robust to outliers compared to MSE (mean squared error). Also, it is easier to interpret in the context of money (dollars off in average transaction value). However, it does not penalize large errors similar to MSE.\n\n10-Fold Cross Validation\n\n\nset.seed(244)\n\nmodel_1_cv &lt;- lm_spec %&gt;%\n  fit_resamples(\n    avg_transaction_value ~ age + gender + region_category + membership_category,\n    resamples = vfold_cv(data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nmodel_2_cv &lt;- lm_spec %&gt;%\n  fit_resamples(\n    avg_transaction_value ~ avg_time_spent + avg_frequency_login_days + days_since_last_login + points_in_wallet,\n    resamples = vfold_cv(data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nmodel_3_cv &lt;- lm_spec %&gt;%\n  fit_resamples(\n    avg_transaction_value ~ age * gender + avg_time_spent + membership_category + past_complaint + internet_option + points_in_wallet,\n    resamples = vfold_cv(data, v = 10),\n    metrics = metric_set(mae)\n  )\n\n\nEvaluation Metrics\n\n\nmae_1_in &lt;- model_1 %&gt;%\n  predict(new_data = data) %&gt;%\n  bind_cols(data) %&gt;%\n  mae(truth = avg_transaction_value, estimate = .pred)\n\nmae_2_in &lt;- model_2 %&gt;%\n  predict(new_data = data) %&gt;%\n  bind_cols(data) %&gt;%\n  mae(truth = avg_transaction_value, estimate = .pred)\n\nmae_3_in &lt;- model_3 %&gt;%\n  predict(new_data = data) %&gt;%\n  bind_cols(data) %&gt;%\n  mae(truth = avg_transaction_value, estimate = .pred)\n\n\n# 10-fold CV MAE\ncv_1 &lt;- model_1_cv %&gt;% collect_metrics() %&gt;% filter(.metric == \"mae\")\ncv_2 &lt;- model_2_cv %&gt;% collect_metrics() %&gt;% filter(.metric == \"mae\")\ncv_3 &lt;- model_3_cv %&gt;% collect_metrics() %&gt;% filter(.metric == \"mae\")\n\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n15142.24\n15147.69\n\n\nmodel_2\n15177.9\n15182.68\n\n\nmodel_3\n15132.07\n15141.8\n\n\n\n\nDifferent k\n\n(LOOCV could not be run because this dataset is very large.)\n\nset.seed(244)\ncv3  &lt;- vfold_cv(data, v = 3)\ncv5  &lt;- vfold_cv(data, v = 5)\n\n\ncv_model_3_k3 &lt;- lm_spec %&gt;% \n  fit_resamples( avg_transaction_value ~ age * gender + avg_time_spent +\n                   membership_category + past_complaint + internet_option +\n                   points_in_wallet, resamples = cv3, metrics = metric_set(mae))\n\ncv_model_3_k5 &lt;- lm_spec %&gt;% \n  fit_resamples(avg_transaction_value ~ age * gender + avg_time_spent +\n                  membership_category + past_complaint + internet_option +\n                  points_in_wallet, resamples = cv5, metrics = metric_set(mae))\n\n\nmae_k3  &lt;- collect_metrics(cv_model_3_k3)  %&gt;% filter(.metric == \"mae\")\nmae_k5  &lt;- collect_metrics(cv_model_3_k5)  %&gt;% filter(.metric == \"mae\")\n\ntibble(\n  `CV Type` = c(\"3-Fold\", \"5-Fold\"),\n  `CV MAE` = round(c(\n    as.numeric(mae_k3[1, \"mean\"]),\n    as.numeric(mae_k5[1, \"mean\"])\n  ), 2)\n)\n\n# A tibble: 2 × 2\n  `CV Type` `CV MAE`\n  &lt;chr&gt;        &lt;dbl&gt;\n1 3-Fold      15139.\n2 5-Fold      15142.\n\n\n10-fold cross-validation produced the smallest CV MAE among the values of k = 3, 5, 10. LOOCV could not be run due to the size of the dataset.\n\nModel Selection\n\nWe selected the following linear model based on the lowest cross-validation error:\n\\[\\begin{align*}\n\\text{avg\\_transaction\\_value}_i &= \\beta_0\n+ \\beta_1 \\cdot \\text{age}_i\n+ \\beta_2 \\cdot \\text{gender}_i\n+ \\beta_3 \\cdot (\\text{age}_i \\times \\text{gender}_i) \\\\\n&+ \\beta_4 \\cdot \\text{avg\\_time\\_spent}_i\n+ \\beta_5 \\cdot \\text{membership\\_category}_i\n+ \\beta_6 \\cdot \\text{past\\_complaint}_i \\\\\n&+ \\beta_7 \\cdot \\text{internet\\_option}_i\n+ \\beta_8 \\cdot \\text{points\\_in\\_wallet}_i\n+ \\varepsilon_i\n\\end{align*}\\]\n\n\nPart 2: Logistic Regression\n\nModel & Predictors\n\nWe model the expected average transaction value using the following predictors:\n\nFormula\n\nWe model the log-odds of being high churn risk (\\(Y = 1\\)) as:\n\\[\n\\log\\left( \\frac{\\mathbb{P}(Y = 1 \\mid X_1, \\ldots, X_5)}{1 - \\mathbb{P}(Y = 1 \\mid X_1, \\ldots, X_5)} \\right)\n= \\beta_0 + \\beta_1 \\cdot \\mathrm{age} + \\beta_2 \\cdot \\mathrm{gender} +\n\\beta_3 \\cdot \\mathrm{points\\_in\\_wallet} + \\beta_4 \\cdot \\mathrm{avg\\_time\\_spent} +\n\\beta_5 \\cdot \\mathrm{membership\\_category}\n\\]\n\nInterpretations\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    churn_high = ifelse(churn_risk_score &gt;= 4, 1, 0),\n    gender = factor(gender),\n    membership_category = factor(membership_category)\n  )\n\n\nlogit_model &lt;- glm(\n  churn_high ~ age + gender + points_in_wallet + avg_time_spent + membership_category, data = data, family = binomial(link = \"logit\")\n)\n\n\nsummary(logit_model)\n\n\nCall:\nglm(formula = churn_high ~ age + gender + points_in_wallet + \n    avg_time_spent + membership_category, family = binomial(link = \"logit\"), \n    data = data)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.51662  -0.00005   0.00004   0.00005   2.61789  \n\nCoefficients:\n                                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                             2.281e+01  2.682e+02   0.085   0.9322\nage                                     2.758e-03  1.586e-03   1.739   0.0821\ngenderM                                -5.864e-02  5.074e-02  -1.156   0.2478\ngenderUnknown                          -1.552e+00  1.066e+00  -1.455   0.1456\npoints_in_wallet                       -3.369e-03  1.739e-04 -19.379   &lt;2e-16\navg_time_spent                         -8.172e-05  6.442e-05  -1.269   0.2046\nmembership_categoryGold Membership     -2.091e+01  2.682e+02  -0.078   0.9379\nmembership_categoryNo Membership       -7.750e-03  3.788e+02   0.000   1.0000\nmembership_categoryPlatinum Membership -4.101e+01  4.478e+02  -0.092   0.9270\nmembership_categoryPremium Membership  -4.104e+01  4.435e+02  -0.093   0.9263\nmembership_categorySilver Membership   -2.068e+01  2.682e+02  -0.077   0.9385\n                                          \n(Intercept)                               \nage                                    .  \ngenderM                                   \ngenderUnknown                             \npoints_in_wallet                       ***\navg_time_spent                            \nmembership_categoryGold Membership        \nmembership_categoryNo Membership          \nmembership_categoryPlatinum Membership    \nmembership_categoryPremium Membership     \nmembership_categorySilver Membership      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27281.5  on 19881  degrees of freedom\nResidual deviance:  8835.6  on 19871  degrees of freedom\nAIC: 8857.6\n\nNumber of Fisher Scoring iterations: 19\n\n\n\nage: For each 1-year increase in age, the odds of high churn risk increase slightly.\ngenderMale: Being male is associated with slightly lower odds of high churn risk compared to being female.\ngenderUnknown: Having unknown gender is associated with much lower odds of high churn risk compared to being female.\npoints_in_wallet: For each additional point in the wallet, the odds of high churn risk decrease slightly.\navg_time_spent: For each additional unit of average time spent, the odds of high churn risk decrease very slightly.\nGold Membership: Being a Gold member is associated with very low odds of high churn risk compared to Basic.\nNo Membership: Having no membership is associated with little to no change in churn risk compared to Basic.\nPlatinum Membership: Being a Platinum member is associated with very low odds of high churn risk compared to Basic.\nPremium Membership: Being a Premium member is associated with very low odds of high churn risk compared to Basic.\nSilver Membership: Being a Silver member is associated with very low odds of high churn risk compared to Basic.\n\n\nMathematical Calculation\n\n\\[\n\\hat{\\beta}_{\\mathrm{points\\_in\\_wallet}} = -0.003336\n\\]\n\nexp(-0.003336)\n\n[1] 0.9966696\n\n\nEach 1-point increase multiplies the odds of high churn by 0.9967.\n\nPrediction\n\n\nnew_data &lt;- data.frame(\n  age = c(25, 45),\n  gender = factor(c(\"F\", \"M\"), levels = levels(data$gender)),\n  points_in_wallet = c(100, 300),\n  avg_time_spent = c(20, 5),\n  membership_category = factor(c(\"Basic Membership\", \"Gold Membership\"),\n                               levels = levels(data$membership_category))\n)\n\npredicted_probs &lt;- predict(logit_model, newdata = new_data, type = \"response\")\n\ncbind(new_data, predicted_probability = predicted_probs)\n\n  age gender points_in_wallet avg_time_spent membership_category\n1  25      F              100             20    Basic Membership\n2  45      M              300              5     Gold Membership\n  predicted_probability\n1             1.0000000\n2             0.7215578\n\n\nA 25-year-old female with 100 wallet points, 20 average time spent, and Basic Membership has a predicted probability of 1 of being high churn risk.\nA 45-year-old male with 300 wallet points, 5 average time spent, and Gold Membership has a predicted probability of 0.712 of being high churn risk.\n\nPros & Cons\n\nLogistic regression is better suited for binary outcomes because it ensures predicted probabilities stay between 0 and 1, while linear regression can produce invalid probabilities outside that range. Logistic regression models the log-odds, but its coefficients are harder to interpret."
  },
  {
    "objectID": "FPCP2.html",
    "href": "FPCP2.html",
    "title": "STAT244 Final Project",
    "section": "",
    "text": "The data set includes user demographic information, browsing behavior, and historical data, which will be used to predict customer churn scores. It includes 36,992 training examples with 25 features and 19,919 test examples with 24 features.\nThe data was downloaded from Kaggle and is a part of a hackathon conducted by HackerEarth. The link to the data set is https://www.kaggle.com/datasets/imsparsh/churn-risk-rate-hackerearth-ml/?select=train.csv. I most recently accessed the data on April 28, 2025.\nThere is no specific information provided regarding the sampling method or the year the data was collected. Therefore, it is assumed to be unknown. Furthermore, it is unlikely that the data collectors had an ulterior motive, as the data was gathered for the purposes of a machine learning competition to help participants predict customer churn. However, in competition data, there could be intentional simplifications or biases, which were not stated in the data description. The source still appears to be reliable, as it was hosted on a official platform.\n\ndata &lt;- read.csv(\"customer.csv\")\n\n\nnrow(data)\n\n[1] 24853\n\n\n\n\n\n\ncolnames(data)\n\n [1] \"customer_id\"                  \"Name\"                        \n [3] \"age\"                          \"gender\"                      \n [5] \"security_no\"                  \"region_category\"             \n [7] \"membership_category\"          \"joining_date\"                \n [9] \"joined_through_referral\"      \"referral_id\"                 \n[11] \"preferred_offer_types\"        \"medium_of_operation\"         \n[13] \"internet_option\"              \"last_visit_time\"             \n[15] \"days_since_last_login\"        \"avg_time_spent\"              \n[17] \"avg_transaction_value\"        \"avg_frequency_login_days\"    \n[19] \"points_in_wallet\"             \"used_special_discount\"       \n[21] \"offer_application_preference\" \"past_complaint\"              \n[23] \"complaint_status\"             \"feedback\"                    \n[25] \"churn_risk_score\"             \"churn_high\"                  \n\n\nI will be considering 7 numerical variables and 12 categorical variables. They are listed below along.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach row in the data set represents one individual customer who has engaged with the platform and has at least one recorded purchase."
  },
  {
    "objectID": "FPCP2.html#data-source-and-description",
    "href": "FPCP2.html#data-source-and-description",
    "title": "STAT244 Final Project",
    "section": "",
    "text": "The data set includes user demographic information, browsing behavior, and historical data, which will be used to predict customer churn scores. It includes 36,992 training examples with 25 features and 19,919 test examples with 24 features.\nThe data was downloaded from Kaggle and is a part of a hackathon conducted by HackerEarth. The link to the data set is https://www.kaggle.com/datasets/imsparsh/churn-risk-rate-hackerearth-ml/?select=train.csv. I most recently accessed the data on April 28, 2025.\nThere is no specific information provided regarding the sampling method or the year the data was collected. Therefore, it is assumed to be unknown. Furthermore, it is unlikely that the data collectors had an ulterior motive, as the data was gathered for the purposes of a machine learning competition to help participants predict customer churn. However, in competition data, there could be intentional simplifications or biases, which were not stated in the data description. The source still appears to be reliable, as it was hosted on a official platform.\n\ndata &lt;- read.csv(\"customer.csv\")\n\n\nnrow(data)\n\n[1] 24853"
  },
  {
    "objectID": "FPCP2.html#variables-of-interest",
    "href": "FPCP2.html#variables-of-interest",
    "title": "STAT244 Final Project",
    "section": "",
    "text": "colnames(data)\n\n [1] \"customer_id\"                  \"Name\"                        \n [3] \"age\"                          \"gender\"                      \n [5] \"security_no\"                  \"region_category\"             \n [7] \"membership_category\"          \"joining_date\"                \n [9] \"joined_through_referral\"      \"referral_id\"                 \n[11] \"preferred_offer_types\"        \"medium_of_operation\"         \n[13] \"internet_option\"              \"last_visit_time\"             \n[15] \"days_since_last_login\"        \"avg_time_spent\"              \n[17] \"avg_transaction_value\"        \"avg_frequency_login_days\"    \n[19] \"points_in_wallet\"             \"used_special_discount\"       \n[21] \"offer_application_preference\" \"past_complaint\"              \n[23] \"complaint_status\"             \"feedback\"                    \n[25] \"churn_risk_score\"             \"churn_high\"                  \n\n\nI will be considering 7 numerical variables and 12 categorical variables. They are listed below along."
  },
  {
    "objectID": "FPCP2.html#observational-unit",
    "href": "FPCP2.html#observational-unit",
    "title": "STAT244 Final Project",
    "section": "",
    "text": "Each row in the data set represents one individual customer who has engaged with the platform and has at least one recorded purchase."
  },
  {
    "objectID": "FPCP2.html#variables-formatting",
    "href": "FPCP2.html#variables-formatting",
    "title": "STAT244 Final Project",
    "section": "Variables Formatting",
    "text": "Variables Formatting\nThe variable names in the data set are overall well-structured and easy to use in code. Therefore, renaming the variables is not necessary.\nTo verify that the quantitative variables had the correct data types, we used the class() function.\n\nquantitative_vars &lt;- c(\n  \"age\", \"days_since_last_login\", \"avg_time_spent\", \n  \"avg_transaction_value\", \"avg_frequency_login_days\", \n  \"points_in_wallet\", \"churn_risk_score\"\n)\n\nsapply(data[quantitative_vars], class)\n\n                     age    days_since_last_login           avg_time_spent \n               \"integer\"                \"integer\"                \"numeric\" \n   avg_transaction_value avg_frequency_login_days         points_in_wallet \n               \"numeric\"                \"numeric\"                \"numeric\" \n        churn_risk_score \n               \"integer\" \n\n\nAs we can see, avg_frequency_login_days was stored as a character type, while the other numerical variables were correctly stored as either integer or numeric. We converted avg_frequency_login_days to a numeric type.\n\ndata &lt;- data %&gt;%\n  mutate(avg_frequency_login_days = as.numeric(avg_frequency_login_days))\n\nWe also verified the categorical variables’ types.\n\ncategorical_vars &lt;- c(\n  \"gender\", \"region_category\", \"membership_category\", \"joined_through_referral\",\n  \"preferred_offer_types\", \"medium_of_operation\", \"internet_option\",\n  \"used_special_discount\", \"offer_application_preference\", \"past_complaint\",\n  \"complaint_status\", \"feedback\"\n)\n\nsapply(data[categorical_vars], class)\n\n                      gender              region_category \n                 \"character\"                  \"character\" \n         membership_category      joined_through_referral \n                 \"character\"                  \"character\" \n       preferred_offer_types          medium_of_operation \n                 \"character\"                  \"character\" \n             internet_option        used_special_discount \n                 \"character\"                  \"character\" \noffer_application_preference               past_complaint \n                 \"character\"                  \"character\" \n            complaint_status                     feedback \n                 \"character\"                  \"character\" \n\n\nAll categorical variables were initially stored as character type. We converted them to factor variables.\n\ndata &lt;- data %&gt;%\n  mutate(across(all_of(categorical_vars), as.factor))"
  },
  {
    "objectID": "FPCP2.html#feature-engineering",
    "href": "FPCP2.html#feature-engineering",
    "title": "STAT244 Final Project",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nAt the moment, there is no specific need to create new variables for the purpose of our analysis."
  },
  {
    "objectID": "FPCP2.html#missing-values",
    "href": "FPCP2.html#missing-values",
    "title": "STAT244 Final Project",
    "section": "Missing Values",
    "text": "Missing Values\nAs we investigate the data set, we see that in the variables joined_through_referral and medium_of_operation, the value “?” was used to indicate unknown information. We changed these entries to “Unknown”.\n\ndata &lt;- data %&gt;%\n  mutate(across(\n    c(\"joined_through_referral\", \"medium_of_operation\"),\n    ~ ifelse(. == \"?\", \"Unknown\", .)\n  ))\n\nWe summarized missing values by combining counts of NA values and blank (““) entries. We would filter out rows with missing or blank values from key variables.\n\nmissing_summary &lt;- sapply(data, function(x) {\n  sum(is.na(x)) + sum(x == \"\", na.rm = TRUE)\n})\n\nmissing_summary[missing_summary&gt;0]\n\nnamed integer(0)\n\n\n\nnrow(data)\n\n[1] 24853\n\n\nAfter filtering out the missing values, we maintain 24853 observational units, which is still reasonable to use for analysis. Although the data set documentation did not specify why some values are missing, we might be able to assume that missing values are not completely random. For example, missing values in region_category or preferred_offer_types could result from incomplete personal information at registration. Missing values in avg_frequency_login_days and points_in_wallet are likely from users who are not active on the platform."
  },
  {
    "objectID": "FPCP2.html#summaries",
    "href": "FPCP2.html#summaries",
    "title": "STAT244 Final Project",
    "section": "Summaries",
    "text": "Summaries\nWe included statistical summary for our numerical variables as below.\n\n\n\n\n\nWe also included statistical summaries for some of our categorical variables as below. The summaries are quite insightful. We observe balanced classes in gender, but there might be some imbalances in membership_category and region_category."
  },
  {
    "objectID": "FPCP2.html#visualizations",
    "href": "FPCP2.html#visualizations",
    "title": "STAT244 Final Project",
    "section": "Visualizations",
    "text": "Visualizations\nTo better understand the relationship between user behavior and churn risk, we included a variety of plots."
  },
  {
    "objectID": "FPCP4.html",
    "href": "FPCP4.html",
    "title": "STAT 244-SC Final Project",
    "section": "",
    "text": "Introduction\nThis project explores customer behavior using a dataset with demographic, transactional, and engagement features. There are two main sections. In the first section, we implemented linear regression models to predict each customer’s average transaction value, comparing different model specifications using 10-fold cross-validation. In the second section, we used logistic regression to classify whether a customer is at high churn risk based on selected predictors. The goal is to understand what factors are associated with customer spending and retention, and to evaluate model performance using appropriate validation techniques.\nEach row in the data set represents one individual customer who has engaged with the platform and has at least one recorded purchase.\n\n\nExploratory Data Analysis\nTo better understand the relationship between user behavior and churn risk, we included a variety of plots."
  },
  {
    "objectID": "FPCP4.html#variables-of-interest",
    "href": "FPCP4.html#variables-of-interest",
    "title": "STAT 244-SC Final Project",
    "section": "",
    "text": "I will be considering 7 numerical variables and 12 categorical variables. They are listed below along.\n\n\n\nNumerical Variables\n\n\nVariable\nDescription\n\n\n\n\nage\nThe age of the user, measured in years.\n\n\ndays_since_last_login\nThe number of days since the user last logged into the website, measured in days.\n\n\navg_time_spent\nThe average amount of time the user spends per visit on the website, measured in minutes.\n\n\navg_transaction_value\nThe average value of transactions made by the user, measured in monetary units (currency).\n\n\navg_frequency_login_days\nThe average number of days between the user's consecutive logins, measured in days.\n\n\npoints_in_wallet\nThe amount of loyalty or reward points currently available in the user's wallet, measured in points.\n\n\nchurn_risk_score\nThe churn risk score assigned to the user, ranging from 1 to 5, indicating the likelihood of the user leaving the service (higher scores indicate higher risk).\n\n\n\n\n\n\n\n\n\n\nNumerical Variables\n\n\nVariable\nDescription\n\n\n\n\nage\nThe age of the user, measured in years.\n\n\ndays_since_last_login\nThe number of days since the user last logged into the website, measured in days.\n\n\navg_time_spent\nThe average amount of time the user spends per visit on the website, measured in minutes.\n\n\navg_transaction_value\nThe average value of transactions made by the user, measured in monetary units (currency).\n\n\navg_frequency_login_days\nThe average number of days between the user's consecutive logins, measured in days.\n\n\npoints_in_wallet\nThe amount of loyalty or reward points currently available in the user's wallet, measured in points.\n\n\nchurn_risk_score\nThe churn risk score assigned to the user, ranging from 1 to 5, indicating the likelihood of the user leaving the service (higher scores indicate higher risk)."
  },
  {
    "objectID": "FPCP4.html#observational-unit",
    "href": "FPCP4.html#observational-unit",
    "title": "STAT 244-SC Final Project",
    "section": "",
    "text": "Each row in the data set represents one individual customer who has engaged with the platform and has at least one recorded purchase.\nWe also verified the categorical variables’ types."
  },
  {
    "objectID": "FPCP4.html#summaries",
    "href": "FPCP4.html#summaries",
    "title": "STAT 244-SC Final Project",
    "section": "Summaries",
    "text": "Summaries\nWe included statistical summary for our numerical variables as below.\n\n\n\n\n\nWe also included statistical summaries for some of our categorical variables as below. The summaries are quite insightful. We observe balanced classes in gender, but there might be some imbalances in membership_category and region_category."
  },
  {
    "objectID": "FPCP4.html#visualizations",
    "href": "FPCP4.html#visualizations",
    "title": "STAT 244-SC Final Project",
    "section": "Visualizations",
    "text": "Visualizations\nTo better understand the relationship between user behavior and churn risk, we included a variety of plots."
  }
]